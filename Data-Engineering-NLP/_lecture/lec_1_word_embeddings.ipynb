{"cells":[{"cell_type":"markdown","metadata":{"id":"NX0d-g0CQ7Ig"},"source":["# An interactive introduction to word embeddings\n","\n","## Goals\n","\n","- Demystify text-based AI models\n","- Convince you that this is very cool!\n","\n","## Applications\n","\n","- Translation (eg. Google Translate)\n","- Text recommendation (autocomplete)\n","- Chatbots (automatic customer service)\n","- Much much more!\n","\n","- [See here for state of the art on tasks](https://github.com/sebastianruder/NLP-progress)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzpoRBOhQ7Ik"},"outputs":[],"source":["# setup for the lecture\n","import pandas as pd\n","import scipy as sc\n","import sklearn\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import SGDRegressor\n","from sklearn.metrics import r2_score\n","import statsmodels.api as sm\n","import sys\n","### Gensim is outside the anaconda distribution ###\n","### uncomment to install Gensim ###\n","#!{sys.executable} -m pip install gensim\n","import gensim\n","import gensim.downloader as model_api\n","\n","# Load pretrained word embeddings\n","# This will download 60mb of data the first time it's loaded\n","word_vectors = model_api.load(\"glove-wiki-gigaword-50\")"]},{"cell_type":"markdown","metadata":{"id":"4CjY-QcAQ7Im"},"source":["First, a **magic trick**!\n","\n","$(Paris - France) + Russia = x$ \n","\n","$Paris + Russia - France = x$\n","\n","which should give us $x = Moscow$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0h6UXXDcQ7Im"},"outputs":[],"source":["# Get the most similar word to an expression\n","word_vectors.most_similar_cosmul(positive=['paris', 'russia'], negative=['france'])\n","\n","#Cosine similarity those percentages"]},{"cell_type":"markdown","metadata":{"id":"uA-t6ZC4Q7In"},"source":["**NLP ADVANTAGE:** It's easy to generate datasets in NLP if you're clever!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_-XlIu_Q7In"},"outputs":[],"source":["word_vectors.most_similar_cosmul(positive=['queen', 'man'], negative=['woman'])"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"QAX0WE-hQ7Io"},"outputs":[],"source":["df = pd.read_csv('../data/airline_tweets.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"C2N8Bl6ZQ7Io"},"source":["## Fundamental Problem\n","\n","_If we want the the text to produce predictions or suggestions, we first need to translate it to a mathematical form._\n","\n","**Naive solution:** Have each document (each review) become a list of the words it contains."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMx6g32DQ7Ip"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFtN-v02Q7Iq"},"outputs":[],"source":["# Naive solution\n","vectorizer = CountVectorizer(max_features=1000)\n","#Setting max feature will reduce the vocab to 1000 features\n","# does this by term frequency - reduces dimensionality \n","\n","X = df['text']\n","y = df['airline_sentiment']\n","\n","X = vectorizer.fit_transform(X)\n","wordLabels = vectorizer.get_feature_names()\n","\n","# Print example of the bag-of-words matrix - OHE of all possible words for each row\n","pd.DataFrame(data=X.toarray(), columns=wordLabels).head()\n","\n","# Remember the word Bag of Words\n","\n","# It is easy to try to **predict a review's rating** with this approach:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2rugi-eQ7Iq"},"outputs":[],"source":["X = df['text']\n","y = df['airline_sentiment']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n","\n","X_train = vectorizer.fit_transform(X_train) #Only fit to Train - prevent data leakage\n","X_test = vectorizer.transform(X_test)\n","\n","log_model = LogisticRegression(max_iter=1000).fit(X_train,y_train)\n","\n","preds = log_model.predict(X_test)\n","\n","print(classification_report(y_test,preds))\n","\n","#NOTE: \n","# To optimize BoW - more pre-processing word cleaning is required \n","# ie. stop word removal and/or stemming/lemmatization \n","# Otherwise words that frequently appear in our documents will have little to no predictive power ('a', 'the', etc.)"]},{"cell_type":"markdown","metadata":{"id":"1alNYvGsQ7Ir"},"source":["## TF-IDF\n","\n","You can also augment the classic bag-of-words with [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"]},{"cell_type":"markdown","metadata":{"id":"UG4d62edQ7Ir"},"source":["TF-IDF: \n","- Looks at work frequency within a document (in this case the comment) and also looks at how much it appears throughout the many rows \n","\n","- Gets a weight on the word to understand the importance or value a word has. Like The - very often used so might not reveal a lot \n","\n","- Looks at a ratio \n","- Frequency is the number of times the term appears in a document\n","- Will decrease the weight of a word that occurs often in the document set while increasing the weight of words that occur less frequently in the set "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TG-5tFd5Q7Ir"},"outputs":[],"source":["# td-idf - term frequency - inverse document frequency\n","\n","vectorizer = CountVectorizer()\n","tf = TfidfTransformer()\n","\n","X = df['text']\n","y = df['airline_sentiment']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n","\n","\n","X_train = vectorizer.fit_transform(X_train) #fit_transform CountVectorizer on training data\n","X_train = tf.fit_transform(X_train)         #fit_transform TfidfTransformer on training data\n","\n","X_test = vectorizer.transform(X_test)       #transform CountVectorizer on testing data\n","X_test = tf.transform(X_test)               #transform TfidfTransformer on testing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5R8wMzhnQ7Is"},"outputs":[],"source":["log_model= LogisticRegression(max_iter=1000)#Create instance of our model\n","\n","log_model.fit(X_train,y_train)              #Fit the model on the data\n","\n","preds = log_model.predict(X_test)\n","\n","print(classification_report(y_test,preds))"]},{"cell_type":"markdown","metadata":{"id":"RBR2bepSQ7Is"},"source":["## TfidfVectorizer - CountVectorizer & TfidfTransformer in one step!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TN6ykpfQ7Is"},"outputs":[],"source":["tf = TfidfVectorizer()                #Create an instance of our TfidfVectorize()\n","\n","X = df['text']\n","y = df['airline_sentiment']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n","\n","X_train = tf.fit_transform(X_train)  #fit_transform on training data\n","X_test = tf.transform(X_test)        #transform on testing data "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTVWwBhkQ7Is"},"outputs":[],"source":["log_model = LogisticRegression(max_iter=1000)\n","\n","log_model.fit(X_train,y_train)           \n","\n","preds = log_model.predict(X_test)\n","\n","print(classification_report(y_test,preds))"]},{"cell_type":"markdown","metadata":{"id":"RIfDkOMPQ7It"},"source":["Some fixes for bag-of-words approach are detailed in the first week of [this free NLP course](https://www.coursera.org/learn/language-processing/)."]},{"cell_type":"markdown","metadata":{"id":"KtEQV-bGQ7It"},"source":["## Dimensionality Reduction with some PCA "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"it5ckseoQ7It"},"outputs":[],"source":["X = df['text']\n","y = df['airline_sentiment']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaVNEDleQ7It"},"outputs":[],"source":["#Perform Train Test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOpSc8UtQ7It"},"outputs":[],"source":["# Apply our TfidfVectorizer to our data toarray() - PCA does not support sparse input\n","tf = TfidfVectorizer()\n","\n","tf_X_train = tf.fit_transform(X_train).toarray()\n","tf_X_test = tf.transform(X_test).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sY2CtQL1Q7It"},"outputs":[],"source":["COMPRESSED_SIZE = 200            #Firs 200 principle components \n","\n","pca_model = PCA(COMPRESSED_SIZE) # create instance of our PCA model "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa-7Rw6JQ7Iu"},"outputs":[],"source":["# - ensure input is not sparse \n","\n","pca_model.fit(tf_X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3gEBRiEQ7Iu"},"outputs":[],"source":["#transform both the tf_X_train and tf_X_test - ensure input is not sparse \n","\n","pca_train = pca_model.transform(tf_X_train) \n","pca_test = pca_model.transform(tf_X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IirvubMNQ7Iu"},"outputs":[],"source":["tf_X_train.shape, pca_train.shape #Same number of rows, feature dimensions are reduced \n","\n","#NOTE: \n","# Principal Component is a linear combination of original features\n","\n","# 100% of the variance in the data is explained by all original features....\n","# We trade off some of the explained variance for less dimensions\n","# This can be significant savings for data sets with MANY dimensions but only a few strong features  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HShrx-WoQ7Iu"},"outputs":[],"source":["#Time for modelling - create a logistic_reg model with max_iter set to 1000 \n","\n","model = LogisticRegression(max_iter=1000)\n","model.fit(pca_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OWzl-rBQ7Iu"},"outputs":[],"source":["#Generate predictions based on our pca_test set\n","preds = model.predict(pca_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NkYWjxzXQ7Iu"},"outputs":[],"source":["print(classification_report(y_test,preds))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yir9Ep4hQ7Iu"},"outputs":[],"source":["#Percentage of explained variance for each new dimensions\n","\n","pca_model.explained_variance_ratio_[:10]"]},{"cell_type":"markdown","metadata":{"id":"Nhr16T_EQ7Iu"},"source":["# What are the problems with this approach?\n","\n","- Doesn't associate similar/same words\n","\n","\n","- No information about words themselves\n","\n","\n","- No word importance information\n","\n","Some fixes for bag-of-words approach are detailed in the first week of [this free NLP course](https://www.coursera.org/learn/language-processing/)."]},{"cell_type":"markdown","metadata":{"id":"vKaCPMTVQ7Iw"},"source":["\n","\n","There are many better methods to generate embeddings.\n","\n","- The most popular is [word2vec](https://www.tensorflow.org/tutorials/representation/word2vec) and [GloVE](https://nlp.stanford.edu/projects/glove/).\n","- There are also methods based on [matrix factorization](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/) like we did.\n","- Modern techniques use recurrent neural net models [predicting words](https://thegradient.pub/nlp-imagenet/) to generate better embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZqFVg4tQ7Iw"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"collapsed_sections":["RBR2bepSQ7Is","KtEQV-bGQ7It"]}},"nbformat":4,"nbformat_minor":0}