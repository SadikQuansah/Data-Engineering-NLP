{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8979gHRz06dT"
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "### Gensim is outside the anaconda distribution ###\n",
    "### uncomment to install Gensim ###\n",
    "#!{sys.executable} -m pip install gensim\n",
    "import gensim\n",
    "import gensim.downloader as model_api\n",
    "\n",
    "# Load pretrained word embeddings\n",
    "# This will download 60mb of data the first time it's loaded\n",
    "word_vectors = model_api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V84L-cAR06dc"
   },
   "source": [
    "## Sentence embeddings\n",
    "\n",
    "There is some good information decomposing word embeddings on [Jay Alammar's blog](http://jalammar.github.io/illustrated-word2vec/).\n",
    "\n",
    "Word embedding dimensions capture high level concepts, which let algebra \"work\" in cosine distance.\n",
    "\n",
    "The simplest and most effective way to represent a sentence is to sum or average the sentence's words. There are [some better methods](https://openreview.net/forum?id=SyK00v5xx) using weights, or using deep learning language models, but sentence embeddings are often just as good while being simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKwJoLUb06dc",
    "outputId": "e825e51a-9563-481f-c6fc-404d550866b0"
   },
   "outputs": [],
   "source": [
    "# sentence embedding\n",
    "\n",
    "df = pd.read_csv('../data/Restaurant_Reviews.tsv',delimiter='\\t')\n",
    "df.columns = [x.lower() for x in df.columns]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jX2Qda7906dd"
   },
   "outputs": [],
   "source": [
    "# split the words\n",
    "\n",
    "words = df.review.str.split()\n",
    "words = pd.DataFrame(words.tolist())\n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQpoflNi06dd",
    "outputId": "4701fdef-7aaa-4d9e-f6dc-7755c0a694b5"
   },
   "outputs": [],
   "source": [
    "# clean up the words with regex \n",
    "import re\n",
    "\n",
    "replaceDict = dict({\n",
    "'{':\" \", '}':\" \", ',':\"\", '.':\" \", '!':\" \", '\\\\':\" \", '/':\" \", '$':\" \", '%':\" \",\n",
    "'^':\" \", '?':\" \", '\\'':\" \", '\"':\" \", '(':\" \", ')':\" \", '*':\" \", '+':\" \", '-':\" \",\n",
    "'=':\" \", ':':\" \", ';':\" \", ']':\" \", '[':\" \", '`':\" \", '~':\" \",\n",
    "})\n",
    "\n",
    "rep = dict((re.escape(k),v) for k, v in replaceDict.items())\n",
    "pattern = re.compile('|'.join(rep.keys()))\n",
    "def replacer(text):\n",
    "    return rep[re.escape(text.group(0))]\n",
    "\n",
    "words = df.review.str.replace(pattern, replacer).str.lower().str.split()\n",
    "words = pd.DataFrame(words.tolist())\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIjysH7H06de"
   },
   "source": [
    "### Sentence embeddings quickly\n",
    "\n",
    "This is a short way to generate sentence embeddings from a column.\n",
    "\n",
    "It's not very efficient but can be optimized a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqYPdHh106de",
    "outputId": "429059c2-aed5-4496-a308-cf1565b6893a"
   },
   "outputs": [],
   "source": [
    "def soft_get(w):\n",
    "    try:\n",
    "        return word_vectors[w] #either get the word or return 0s\n",
    "    except KeyError:\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "\n",
    "def map_vectors(row):\n",
    "    try:\n",
    "        return np.sum(\n",
    "            row.loc[words.iloc[0].notna()].apply(soft_get)\n",
    "        ) # take the row and take the columns that are not NaN and get the soft_get and then take the sum of that\n",
    "    except:\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "\n",
    "emb = pd.DataFrame(words.apply(map_vectors, axis=1).tolist())\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "sm.OLS(df.liked, sm.add_constant(emb)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_pretrained_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
